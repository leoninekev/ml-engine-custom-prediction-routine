{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_prediction_model_deployment_Walkthrough.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoninekev/ml-engine-custom-prediction-routine/blob/master/custom_prediction_model_deployment_Walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhYHxegHfHaq",
        "colab_type": "text"
      },
      "source": [
        " ## Custom prediction routines "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWqKXHk2hfdW",
        "colab_type": "text"
      },
      "source": [
        "**Model deployment task in google Ai-latform necessitates compliance, completion of following three sub-rotines:**\n",
        "* Upload of model artifacts to cloud storage bucket (Artifacts include - saved model/keras Model file, custom prediction, pre/post processing scripts, confi files)\n",
        "* Creation of ai-platform **Model Resource**.\n",
        "* Creation of ai-platform **Version Resource** (Specifying path to cloud storage holding Model & artifact package)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfG0jM-VfAnM",
        "colab_type": "text"
      },
      "source": [
        "###  Custom prediction routines determines, what code runs when an online prediction request to ai-platform is made.\n",
        "\n",
        "Deploying a custom prediction routine as **Version Resource** serves many utilities as follows:\n",
        "* It enables AI Platform to run a custom python code in response to each incoming request received for prediction/inferencing.\n",
        "* It allows preprocessing of input data before it is forwarded to a trained model for prediction.\n",
        "* It also allows postprocessing model's output prior to actual posting of prediction result, thus modifying output to suit any application's endpoints requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXLnv5uHdgDW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7GtubewdfO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "afb40e75-6422-4eb7-b06e-ac4a47a3e2bf"
      },
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0730 10:14:37.331681 140175376680832 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyzHA0XkUkbO",
        "colab_type": "code",
        "outputId": "271de867-58f8-4fbb-85f7-fae135391392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "PROJECT_ID = \"nifty-episode-231612\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "!gcloud config list"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "[component_manager]\n",
            "disable_update_check = True\n",
            "[core]\n",
            "account = quantumbisht@gmail.com\n",
            "project = nifty-episode-231612\n",
            "\n",
            "Your active configuration is: [default]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmadMp0pdhC6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpDybFraCgfO",
        "colab_type": "code",
        "outputId": "2b5d8065-b3e0-4935-bd5c-54cb4854b62e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!git clone https://github.com/leoninekev/ml-engine-custom-prediction-routine.git\n",
        "  \n",
        "%cd ml-engine-custom-prediction-routine/\n",
        "\n",
        "! ls -pR"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ml-engine-custom-prediction-routine'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/49)   \u001b[K\rremote: Counting objects:   4% (2/49)   \u001b[K\rremote: Counting objects:   6% (3/49)   \u001b[K\rremote: Counting objects:   8% (4/49)   \u001b[K\rremote: Counting objects:  10% (5/49)   \u001b[K\rremote: Counting objects:  12% (6/49)   \u001b[K\rremote: Counting objects:  14% (7/49)   \u001b[K\rremote: Counting objects:  16% (8/49)   \u001b[K\rremote: Counting objects:  18% (9/49)   \u001b[K\rremote: Counting objects:  20% (10/49)   \u001b[K\rremote: Counting objects:  22% (11/49)   \u001b[K\rremote: Counting objects:  24% (12/49)   \u001b[K\rremote: Counting objects:  26% (13/49)   \u001b[K\rremote: Counting objects:  28% (14/49)   \u001b[K\rremote: Counting objects:  30% (15/49)   \u001b[K\rremote: Counting objects:  32% (16/49)   \u001b[K\rremote: Counting objects:  34% (17/49)   \u001b[K\rremote: Counting objects:  36% (18/49)   \u001b[K\rremote: Counting objects:  38% (19/49)   \u001b[K\rremote: Counting objects:  40% (20/49)   \u001b[K\rremote: Counting objects:  42% (21/49)   \u001b[K\rremote: Counting objects:  44% (22/49)   \u001b[K\rremote: Counting objects:  46% (23/49)   \u001b[K\rremote: Counting objects:  48% (24/49)   \u001b[K\rremote: Counting objects:  51% (25/49)   \u001b[K\rremote: Counting objects:  53% (26/49)   \u001b[K\rremote: Counting objects:  55% (27/49)   \u001b[K\rremote: Counting objects:  57% (28/49)   \u001b[K\rremote: Counting objects:  59% (29/49)   \u001b[K\rremote: Counting objects:  61% (30/49)   \u001b[K\rremote: Counting objects:  63% (31/49)   \u001b[K\rremote: Counting objects:  65% (32/49)   \u001b[K\rremote: Counting objects:  67% (33/49)   \u001b[K\rremote: Counting objects:  69% (34/49)   \u001b[K\rremote: Counting objects:  71% (35/49)   \u001b[K\rremote: Counting objects:  73% (36/49)   \u001b[K\rremote: Counting objects:  75% (37/49)   \u001b[K\rremote: Counting objects:  77% (38/49)   \u001b[K\rremote: Counting objects:  79% (39/49)   \u001b[K\rremote: Counting objects:  81% (40/49)   \u001b[K\rremote: Counting objects:  83% (41/49)   \u001b[K\rremote: Counting objects:  85% (42/49)   \u001b[K\rremote: Counting objects:  87% (43/49)   \u001b[K\rremote: Counting objects:  89% (44/49)   \u001b[K\rremote: Counting objects:  91% (45/49)   \u001b[K\rremote: Counting objects:  93% (46/49)   \u001b[K\rremote: Counting objects:  95% (47/49)   \u001b[K\rremote: Counting objects:  97% (48/49)   \u001b[K\rremote: Counting objects: 100% (49/49)   \u001b[K\rremote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/30)   \u001b[K\rremote: Compressing objects:   6% (2/30)   \u001b[K\rremote: Compressing objects:  10% (3/30)   \u001b[K\rremote: Compressing objects:  13% (4/30)   \u001b[K\rremote: Compressing objects:  16% (5/30)   \u001b[K\rremote: Compressing objects:  20% (6/30)   \u001b[K\rremote: Compressing objects:  23% (7/30)   \u001b[K\rremote: Compressing objects:  26% (8/30)   \u001b[K\rremote: Compressing objects:  30% (9/30)   \u001b[K\rremote: Compressing objects:  33% (10/30)   \u001b[K\rremote: Compressing objects:  36% (11/30)   \u001b[K\rremote: Compressing objects:  40% (12/30)   \u001b[K\rremote: Compressing objects:  43% (13/30)   \u001b[K\rremote: Compressing objects:  46% (14/30)   \u001b[K\rremote: Compressing objects:  50% (15/30)   \u001b[K\rremote: Compressing objects:  53% (16/30)   \u001b[K\rremote: Compressing objects:  56% (17/30)   \u001b[K\rremote: Compressing objects:  60% (18/30)   \u001b[K\rremote: Compressing objects:  63% (19/30)   \u001b[K\rremote: Compressing objects:  66% (20/30)   \u001b[K\rremote: Compressing objects:  70% (21/30)   \u001b[K\rremote: Compressing objects:  73% (22/30)   \u001b[K\rremote: Compressing objects:  76% (23/30)   \u001b[K\rremote: Compressing objects:  80% (24/30)   \u001b[K\rremote: Compressing objects:  83% (25/30)   \u001b[K\rremote: Compressing objects:  86% (26/30)   \u001b[K\rremote: Compressing objects:  90% (27/30)   \u001b[K\rremote: Compressing objects:  93% (28/30)   \u001b[K\rremote: Compressing objects:  96% (29/30)   \u001b[K\rremote: Compressing objects: 100% (30/30)   \u001b[K\rremote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "Unpacking objects:   2% (1/49)   \rUnpacking objects:   4% (2/49)   \rUnpacking objects:   6% (3/49)   \rUnpacking objects:   8% (4/49)   \rUnpacking objects:  10% (5/49)   \rUnpacking objects:  12% (6/49)   \rUnpacking objects:  14% (7/49)   \rUnpacking objects:  16% (8/49)   \rUnpacking objects:  18% (9/49)   \rUnpacking objects:  20% (10/49)   \rUnpacking objects:  22% (11/49)   \rUnpacking objects:  24% (12/49)   \rUnpacking objects:  26% (13/49)   \rUnpacking objects:  28% (14/49)   \rUnpacking objects:  30% (15/49)   \rUnpacking objects:  32% (16/49)   \rUnpacking objects:  34% (17/49)   \rUnpacking objects:  36% (18/49)   \rUnpacking objects:  38% (19/49)   \rUnpacking objects:  40% (20/49)   \rUnpacking objects:  42% (21/49)   \rUnpacking objects:  44% (22/49)   \rUnpacking objects:  46% (23/49)   \rUnpacking objects:  48% (24/49)   \rUnpacking objects:  51% (25/49)   \rUnpacking objects:  53% (26/49)   \rUnpacking objects:  55% (27/49)   \rUnpacking objects:  57% (28/49)   \rUnpacking objects:  59% (29/49)   \rUnpacking objects:  61% (30/49)   \rUnpacking objects:  63% (31/49)   \rUnpacking objects:  65% (32/49)   \rremote: Total 49 (delta 20), reused 41 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects:  67% (33/49)   \rUnpacking objects:  69% (34/49)   \rUnpacking objects:  71% (35/49)   \rUnpacking objects:  73% (36/49)   \rUnpacking objects:  75% (37/49)   \rUnpacking objects:  77% (38/49)   \rUnpacking objects:  79% (39/49)   \rUnpacking objects:  81% (40/49)   \rUnpacking objects:  83% (41/49)   \rUnpacking objects:  85% (42/49)   \rUnpacking objects:  87% (43/49)   \rUnpacking objects:  89% (44/49)   \rUnpacking objects:  91% (45/49)   \rUnpacking objects:  93% (46/49)   \rUnpacking objects:  95% (47/49)   \rUnpacking objects:  97% (48/49)   \rUnpacking objects: 100% (49/49)   \rUnpacking objects: 100% (49/49), done.\n",
            "/content/ml-engine-custom-prediction-routine\n",
            ".:\n",
            "config.pickle\t\t\t\t\t      resnet.py\n",
            "custom_prediction_model_deployment_Walkthrough.ipynb  roi_helpers.py\n",
            "FixedBatchNormalization.py\t\t\t      RoiPoolingConv.py\n",
            "LICENSE\t\t\t\t\t\t      setup.py\n",
            "predictor.py\t\t\t\t\t      test_img.jpg\n",
            "README.md\t\t\t\t\t      test_json_1706.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnKBSxGLk5DN",
        "colab_type": "text"
      },
      "source": [
        "* View the setup file and edit the package name and version if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNrhnuZMk2Ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat setup.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9aTN9uvebJF",
        "colab_type": "text"
      },
      "source": [
        "* **Packaging the Predictor module and other supporting artifacts/Scripts.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii5z5Ms8d9dj",
        "colab_type": "code",
        "outputId": "1a5e0d4d-5d81-4c6e-9d27-c9220c82e800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "!python setup.py sdist --formats=gztar"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running sdist\n",
            "running egg_info\n",
            "creating test_code_new_model.egg-info\n",
            "writing test_code_new_model.egg-info/PKG-INFO\n",
            "writing dependency_links to test_code_new_model.egg-info/dependency_links.txt\n",
            "writing requirements to test_code_new_model.egg-info/requires.txt\n",
            "writing top-level names to test_code_new_model.egg-info/top_level.txt\n",
            "writing manifest file 'test_code_new_model.egg-info/SOURCES.txt'\n",
            "writing manifest file 'test_code_new_model.egg-info/SOURCES.txt'\n",
            "running check\n",
            "warning: check: missing required meta-data: url\n",
            "\n",
            "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
            "\n",
            "creating test_code_new_model-0.1\n",
            "creating test_code_new_model-0.1/test_code_new_model.egg-info\n",
            "copying files to test_code_new_model-0.1...\n",
            "copying FixedBatchNormalization.py -> test_code_new_model-0.1\n",
            "copying LICENSE -> test_code_new_model-0.1\n",
            "copying README.md -> test_code_new_model-0.1\n",
            "copying RoiPoolingConv.py -> test_code_new_model-0.1\n",
            "copying config.pickle -> test_code_new_model-0.1\n",
            "copying custom_prediction_model_deployment_Walkthrough.ipynb -> test_code_new_model-0.1\n",
            "copying predictor.py -> test_code_new_model-0.1\n",
            "copying resnet.py -> test_code_new_model-0.1\n",
            "copying roi_helpers.py -> test_code_new_model-0.1\n",
            "copying setup.py -> test_code_new_model-0.1\n",
            "copying test_code_new_model.egg-info/PKG-INFO -> test_code_new_model-0.1/test_code_new_model.egg-info\n",
            "copying test_code_new_model.egg-info/SOURCES.txt -> test_code_new_model-0.1/test_code_new_model.egg-info\n",
            "copying test_code_new_model.egg-info/dependency_links.txt -> test_code_new_model-0.1/test_code_new_model.egg-info\n",
            "copying test_code_new_model.egg-info/requires.txt -> test_code_new_model-0.1/test_code_new_model.egg-info\n",
            "copying test_code_new_model.egg-info/top_level.txt -> test_code_new_model-0.1/test_code_new_model.egg-info\n",
            "Writing test_code_new_model-0.1/setup.cfg\n",
            "Creating tar archive\n",
            "removing 'test_code_new_model-0.1' (and everything under it)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToTQfFkWXnWh",
        "colab_type": "text"
      },
      "source": [
        "* **Copy the model file/model weights and packaged python artifacts to model directory in Cloud storage bucket; The path of which will be used to cite trained model during Version resource creation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p43mmRdb0arW",
        "colab_type": "text"
      },
      "source": [
        "* Since the frcnn model was trained in ml-engine before in a different notebook, and the model weights were hosted on dropbox.\n",
        "* downloading model_weights file here to later copy custom code package & model weights together to  model directory in cloud storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQfQtcevzn3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "2d0e63c2-2cd5-496a-c6e9-ebfb804a0616"
      },
      "source": [
        "!wget -O model_frcnn.hdf5 https://www.dropbox.com/s/makxzi5aoe1bfij/model_frcnn.hdf5?dl=0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-30 09:36:32--  https://www.dropbox.com/s/makxzi5aoe1bfij/model_frcnn.hdf5?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:6018:1::a27d:301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/makxzi5aoe1bfij/model_frcnn.hdf5 [following]\n",
            "--2019-07-30 09:36:32--  https://www.dropbox.com/s/raw/makxzi5aoe1bfij/model_frcnn.hdf5\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc95ac0a175fb68a88392ed889e8.dl.dropboxusercontent.com/cd/0/inline/AloRlg89m03ZhwnY0aCAONKf1gj3mtDYITywVum4uKfYLVgkECb0uKEhFxbhw-2Ip87yOECYQuBHcO829oIW0rcUuK5v3Dot10C4jw3z5auCcA/file# [following]\n",
            "--2019-07-30 09:36:32--  https://uc95ac0a175fb68a88392ed889e8.dl.dropboxusercontent.com/cd/0/inline/AloRlg89m03ZhwnY0aCAONKf1gj3mtDYITywVum4uKfYLVgkECb0uKEhFxbhw-2Ip87yOECYQuBHcO829oIW0rcUuK5v3Dot10C4jw3z5auCcA/file\n",
            "Resolving uc95ac0a175fb68a88392ed889e8.dl.dropboxusercontent.com (uc95ac0a175fb68a88392ed889e8.dl.dropboxusercontent.com)... 162.125.3.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc95ac0a175fb68a88392ed889e8.dl.dropboxusercontent.com (uc95ac0a175fb68a88392ed889e8.dl.dropboxusercontent.com)|162.125.3.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AlpKdma0kQMXcXAwLDDWLDgdwea6FsKnIOnNfvN_lpjZesqnGup-XQ2bvC8PXD5hb4bBLjzeumumMhJHY7qFcU8AhMk-NzGLiS49poH94BVDhqg6uYbKXGIoJeWWRlnIiqV8kA12o1tSc_h7PAv-GkdNXgBqoO2buUvzTxifJBtDhU1ML6YA14CZfMYPxRb81-qjO0W959RhS2db6QInY3dtnocPs4JcYqW3CkpMWKDysdtiyrgZPyPoxdAbwMgviOvuBGlEWoS6DWX4tVYDcqI4A666SI4eqsGRDJ9W5uOiNygcMRT1VAJe5sLT94aMVUzUqt9Nc0DSF0TDLPgruFTL/file [following]\n",
            "--2019-07-30 09:36:33--  https://uc95ac0a175fb68a88392ed889e8.dl.dropboxusercontent.com/cd/0/inline2/AlpKdma0kQMXcXAwLDDWLDgdwea6FsKnIOnNfvN_lpjZesqnGup-XQ2bvC8PXD5hb4bBLjzeumumMhJHY7qFcU8AhMk-NzGLiS49poH94BVDhqg6uYbKXGIoJeWWRlnIiqV8kA12o1tSc_h7PAv-GkdNXgBqoO2buUvzTxifJBtDhU1ML6YA14CZfMYPxRb81-qjO0W959RhS2db6QInY3dtnocPs4JcYqW3CkpMWKDysdtiyrgZPyPoxdAbwMgviOvuBGlEWoS6DWX4tVYDcqI4A666SI4eqsGRDJ9W5uOiNygcMRT1VAJe5sLT94aMVUzUqt9Nc0DSF0TDLPgruFTL/file\n",
            "Reusing existing connection to uc95ac0a175fb68a88392ed889e8.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113893120 (109M) [application/octet-stream]\n",
            "Saving to: ‘model_frcnn.hdf5’\n",
            "\n",
            "model_frcnn.hdf5    100%[===================>] 108.62M  42.5MB/s    in 2.6s    \n",
            "\n",
            "2019-07-30 09:36:36 (42.5 MB/s) - ‘model_frcnn.hdf5’ saved [113893120/113893120]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__AxXk3v0R0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "10768132-d03f-4d7a-b4ad-b52412fcc912"
      },
      "source": [
        "!ls -pR"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".:\n",
            "config.pickle\n",
            "custom_prediction_model_deployment_Walkthrough.ipynb\n",
            "dist/\n",
            "FixedBatchNormalization.py\n",
            "LICENSE\n",
            "model_frcnn.hdf5\n",
            "predictor.py\n",
            "README.md\n",
            "resnet.py\n",
            "roi_helpers.py\n",
            "RoiPoolingConv.py\n",
            "setup.py\n",
            "test_code_new_model.egg-info/\n",
            "\n",
            "./dist:\n",
            "test_code_new_model-0.1.tar.gz\n",
            "\n",
            "./test_code_new_model.egg-info:\n",
            "dependency_links.txt  PKG-INFO\trequires.txt  SOURCES.txt  top_level.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S_A1Gdf2wLu",
        "colab_type": "text"
      },
      "source": [
        "*  **Copy model_drcnn.hdf5 and test_code-0.1.tar.gz to model directory(gs://nifty-episode-231612-mlengine/cloud_test_package_repo/)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxj6r8rwT57u",
        "colab_type": "code",
        "outputId": "0fbaedcd-b3fa-4953-8081-a4f67e424ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!gsutil cp dist/test_code_new_model-0.1.tar.gz gs://nifty-episode-231612-mlengine/cloud_test_package_repo/\n",
        "\n",
        "!gsutil cp model_frcnn.hdf5 gs://nifty-episode-231612-mlengine/cloud_test_package_repo/"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://dist/test_code_new_model-0.1.tar.gz [Content-Type=application/x-tar]...\n",
            "-\n",
            "Operation completed over 1 objects/13.2 KiB.                                     \n",
            "Copying file://model_frcnn.hdf5 [Content-Type=application/octet-stream]...\n",
            "\\\n",
            "Operation completed over 1 objects/108.6 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P5WMbDTcwc2",
        "colab_type": "text"
      },
      "source": [
        "**Run following to Create a Model resource**\n",
        "* Define model name and Region.\n",
        "* Also Enable Online prediction logging, to stream logs that contain the **stderr and stdout streams** from your prediction nodes, It proves useful for debugging during version creation and inferencing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi-hX6YbcxQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cceaa632-1cf0-4165-b6fa-f01ba71e6178"
      },
      "source": [
        "MODEL_NAME = \"FoodPredictor_app\"\n",
        "REGION='asia-northeast1'\n",
        "\n",
        "! gcloud beta ai-platform models create $MODEL_NAME \\\n",
        "  --regions $REGION --enable-console-logging"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created ml engine model [projects/nifty-episode-231612/models/FoodPredictor_app].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RgmobirWehg",
        "colab_type": "text"
      },
      "source": [
        "In order to create Version resource to serve predictions, **Ensure the following:**\n",
        "* The model file or model weights file, model config are stored in a model directory (example here, **gs://nifty-episode-231612-mlengine/cloud_test_package_repo/** in Cloud Storage.\n",
        "* The implementation of the predictor interface and other dependencies packaged as custom code previously is also staged in the same model directory.\n",
        "\n",
        "  The structure of your version resource directory in cloud storage bucket therefore should be as:\n",
        "\n",
        "     ``.your-bucket-name/cloud_test_package_repo/``  \n",
        "     ``model_frcnn.hdf5``   \n",
        "     ``model.config(if any)``\n",
        "     ``test_code-0.1.tar.gz``\n",
        "\n",
        "The ``test_code-0.1.tar.gz`` holds the custom pre/post processing code and most importantly the``predictor`` module, for AI Platform to access ``predictor.MyPredictor``.\n",
        "\n",
        "* The **`--trace-log`** flag lets you view Version resource creation logs in the cell below (One can\n",
        "also view logs and other job details in the GCP Console, if you've enbaled **Stackdriver logging service**.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84k3Et8ZvKU_",
        "colab_type": "text"
      },
      "source": [
        "Now to further with creating Version Resource; Ensure following crucial parameters are defined:\n",
        "* **model** = $MODEL_NAME (your model name from Model Resource)\n",
        "* **VERSION_NAME** = 'v1_a' (your version name)\n",
        "* **python-version** = 3.5 (Python version for which the custom code is written)\n",
        "* **runtime-version**= 1.5 (Tensorflow's runtime version)\n",
        "* **origin** = gs://nifty-episode-231612-mlengine/cloud_test_package_repo/ (model directory holdinig model, model artifacts & packaged code)\n",
        "* **package-uris**=  gs://nifty-episode-231612-mlengine/cloud_test_package_repo/test_code_new_model-0.1.tar.gz (your packaged custom predictor interface code)\n",
        "* **prediction-class**=  predictor.MyPredictor (Call to main predictor class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTFbVL7SW0Mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_NAME=\"FoodPredictor_app\"\n",
        "VERSION_NAME='v1_a'\n",
        "\n",
        "!gcloud beta ai-platform versions create $VERSION_NAME --model $MODEL_NAME --python-version 3.5 --runtime-version 1.5 --machine-type mls1-c4-m2 --origin gs://nifty-episode-231612-mlengine/cloud_test_package_repo/ --package-uris gs://nifty-episode-231612-mlengine/cloud_test_package_repo/test_code_new_model-0.1.tar.gz --prediction-class predictor.MyPredictor --trace-log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hncuWNW0831y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a5ae259c-8084-400c-e067-2b711db85414"
      },
      "source": [
        "!gcloud logging logs list"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME\n",
            "projects/nifty-episode-231612/logs/cloudaudit.googleapis.com%2Factivity\n",
            "projects/nifty-episode-231612/logs/master-replica-0\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Fprimary.stderr\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Fprimary.stdout\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_GcloudColab\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_GcloudColab_1\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_GcloudColab_2\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_GcloudColab_3\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_GcloudColab_firse\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_GcloudColab_kuchbhi\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_GoogleColab\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_gcloudColab\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_googlecolab\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_kaafi_late\n",
            "projects/nifty-episode-231612/logs/ml.googleapis.com%2Ftest_job_validate_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWLoBdpiTUK6",
        "colab_type": "text"
      },
      "source": [
        "## Version resource is Created successfully!\n",
        "\n",
        "**Now to Serve prediction:**\n",
        "* Encode an image into base64 and add it to a JSON string\n",
        "* Pass that JSON string to predictor.MyPredictor.predict as in folllowing cell\n",
        "\n",
        "**Recall predict method of class MyPredictor in predictor module**,\n",
        "The image instance passed to it as function argument will be decoded in following manner:\n",
        "\n",
        "* inputs= base64.b64decode(instances['image_bytes']['b64'])\n",
        "\n",
        "* inputs= scipy.misc.imread(io.BytesIO(inputs))\n",
        "\n",
        "**Therefore taking cognizance of the decoding at the predict method end, test image for prediction needs to be encoded in accordance as follows.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ajvzEKvTUtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "with open('test_img.jpg','rb') as image:\n",
        "  img_str= base64.b64encode(image.read())\n",
        "  instances= {'image_bytes': {'b64': base64.b64encode(img_str).decode()}}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxlDuVRcTxYx",
        "colab_type": "text"
      },
      "source": [
        "* Define Project ID, Model name & version of Serving model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-vCqLIKYKSk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0f1291f7-817c-45d0-c157-2d3070f775a1"
      },
      "source": [
        "!gcloud config list"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[component_manager]\n",
            "disable_update_check = True\n",
            "[core]\n",
            "account = quantumbisht@gmail.com\n",
            "project = nifty-episode-231612\n",
            "\n",
            "Your active configuration is: [default]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCfxKLSVXakw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "708fdfea-aa73-43f1-96c8-f0826c94517e"
      },
      "source": [
        "import googleapiclient.discovery\n",
        "service = googleapiclient.discovery.build('ml', 'v1')\n",
        "PROJECT_ID='nifty-episode-231612'\n",
        "MODEL_NAME='FoodPredictor_app'\n",
        "VERSION_NAME='v1_a'\n",
        "\n",
        "name = 'projects/{}/models/{}/versions/{}'.format(PROJECT_ID, MODEL_NAME, VERSION_NAME)\n",
        "\n",
        "response = service.projects().predict(name=name,body={'instances': instances}).execute()\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0730 10:16:53.395616 140175376680832 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "W0730 10:16:53.440198 140175376680832 _default.py:280] No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "projects/nifty-episode-231612/models/FoodPredictor_app/versions/v1_a\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}